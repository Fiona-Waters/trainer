{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DDP Fashion MNIST Training Example run with Local Process Backend\n",
    "\n",
    "This example demonstrates training on your local machine using **native Python processes** (no containers required).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8+ installed\n",
    "- No Docker or Podman required! \n",
    "\n",
    "The notebook demonstrates how to train a convolutional neural network (CNN) to classify images using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and [PyTorch Distributed Data Parallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Kubeflow Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kubeflow-trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Function\n",
    "\n",
    "This function trains a simple CNN on Fashion MNIST dataset using PyTorch.\n",
    "\n",
    "**Note:** LocalProcessBackend runs in a **single process** (no distributed training), so we don't use `torch.distributed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fashion_mnist():\n",
    "    \"\"\"Train a CNN on Fashion MNIST using PyTorch (single process).\"\"\"\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn, optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Simple CNN model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "            self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 4 * 4 * 50)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # Create model\n",
    "    model = Net()\n",
    "    \n",
    "    # Load Fashion MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.FashionMNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Train for 2 epochs\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    \n",
    "    for epoch in range(1, 3):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"fashion_mnist_cnn.pt\")\n",
    "    print(\"Model saved!\")\n",
    "    \n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TrainerClient\n",
    "\n",
    "Create a client with LocalProcessBackend configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient, LocalProcessBackendConfig\n",
    "\n",
    "# Create backend config\n",
    "backend_config = LocalProcessBackendConfig(\n",
    "    cleanup_venv=True  # Automatically clean up virtual environments after jobs complete\n",
    ")\n",
    "\n",
    "# Initialize client\n",
    "client = TrainerClient(backend_config=backend_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the Training Runtimes\n",
    "\n",
    "You can get the list of available Training Runtimes to start your TrainJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='torch-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)\n",
    "    if runtime.name == \"torch-distributed\":\n",
    "        torch_runtime = runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training Job\n",
    "\n",
    "Launch a training job as a local subprocess:\n",
    "\n",
    "This will:\n",
    "- Create a temporary virtual environment\n",
    "- Install required packages (torch, torchvision)\n",
    "- Execute your training function in a subprocess\n",
    "- Clean up the venv automatically when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_fashion_mnist,\n",
    "        packages_to_install=[\"torch\", \"torchvision\"],  # Required packages\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Job Status\n",
    "\n",
    "Check the status of your training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Job Status:\n",
      "   Name: zf5194b02611\n",
      "   Status: Running\n",
      "   Created: 2025-10-21 09:57:03.917775\n",
      "   Steps:\n",
      "     â€¢ train: Running\n"
     ]
    }
   ],
   "source": [
    "# Get job status\n",
    "job = client.get_job(job_name)\n",
    "\n",
    "print(f\"\\n Job Status:\")\n",
    "print(f\"   Name: {job.name}\")\n",
    "print(f\"   Status: {job.status}\")\n",
    "print(f\"   Created: {job.creation_timestamp}\")\n",
    "print(f\"   Steps:\")\n",
    "for step in job.steps:\n",
    "    print(f\"     â€¢ {step.name}: {step.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Training Logs\n",
    "\n",
    "Watch the training progress in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming logs (Ctrl+C to stop):\n",
      "\n",
      "================================================================================\n",
      "Operating inside /var/folders/r3/kwn1z7n15nq3rh54ykdsy73r0000gn/T/zf5194b02611womr11v5\n",
      "Looking in links: /tmp/tmpdcvyyznz\n",
      "Processing /tmp/tmpdcvyyznz/pip-24.2-py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-24.2\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Using cached torch-2.9.0-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached torchvision-0.24.0-cp313-cp313-macosx_12_0_arm64.whl (1.9 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.4 pillow-12.0.0 setuptools-80.9.0 sympy-1.14.0 torch-2.9.0 torchvision-0.24.0 typing-extensions-4.15.0\n",
      "W1021 09:57:20.870000 47481 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "ðŸš€ Starting training...\n",
      "\n",
      "0.1%\n",
      "0.2%\n",
      "0.4%\n",
      "0.5%\n",
      "0.6%\n",
      "0.7%\n",
      "0.9%\n",
      "1.0%\n",
      "1.1%\n",
      "1.2%\n",
      "1.4%\n",
      "1.5%\n",
      "1.6%\n",
      "1.7%\n",
      "1.9%\n",
      "2.0%\n",
      "2.1%\n",
      "2.2%\n",
      "2.4%\n",
      "2.5%\n",
      "2.6%\n",
      "2.7%\n",
      "2.9%\n",
      "3.0%\n",
      "3.1%\n",
      "3.2%\n",
      "3.3%\n",
      "3.5%\n",
      "3.6%\n",
      "3.7%\n",
      "3.8%\n",
      "4.0%\n",
      "4.1%\n",
      "4.2%\n",
      "4.3%\n",
      "4.5%\n",
      "4.6%\n",
      "4.7%\n",
      "4.8%\n",
      "5.0%\n",
      "5.1%\n",
      "5.2%\n",
      "5.3%\n",
      "5.5%\n",
      "5.6%\n",
      "5.7%\n",
      "5.8%\n",
      "6.0%\n",
      "6.1%\n",
      "6.2%\n",
      "6.3%\n",
      "6.4%\n",
      "6.6%\n",
      "6.7%\n",
      "6.8%\n",
      "6.9%\n",
      "7.1%\n",
      "7.2%\n",
      "7.3%\n",
      "7.4%\n",
      "7.6%\n",
      "7.7%\n",
      "7.8%\n",
      "7.9%\n",
      "8.1%\n",
      "8.2%\n",
      "8.3%\n",
      "8.4%\n",
      "8.6%\n",
      "8.7%\n",
      "8.8%\n",
      "8.9%\n",
      "9.1%\n",
      "9.2%\n",
      "9.3%\n",
      "9.4%\n",
      "9.5%\n",
      "9.7%\n",
      "9.8%\n",
      "9.9%\n",
      "10.0%\n",
      "10.2%\n",
      "10.3%\n",
      "10.4%\n",
      "10.5%\n",
      "10.7%\n",
      "10.8%\n",
      "10.9%\n",
      "11.0%\n",
      "11.2%\n",
      "11.3%\n",
      "11.4%\n",
      "11.5%\n",
      "11.7%\n",
      "11.8%\n",
      "11.9%\n",
      "12.0%\n",
      "12.2%\n",
      "12.3%\n",
      "12.4%\n",
      "12.5%\n",
      "12.6%\n",
      "12.8%\n",
      "12.9%\n",
      "13.0%\n",
      "13.1%\n",
      "13.3%\n",
      "13.4%\n",
      "13.5%\n",
      "13.6%\n",
      "13.8%\n",
      "13.9%\n",
      "14.0%\n",
      "14.1%\n",
      "14.3%\n",
      "14.4%\n",
      "14.5%\n",
      "14.6%\n",
      "14.8%\n",
      "14.9%\n",
      "15.0%\n",
      "15.1%\n",
      "15.3%\n",
      "15.4%\n",
      "15.5%\n",
      "15.6%\n",
      "15.8%\n",
      "15.9%\n",
      "16.0%\n",
      "16.1%\n",
      "16.2%\n",
      "16.4%\n",
      "16.5%\n",
      "16.6%\n",
      "16.7%\n",
      "16.9%\n",
      "17.0%\n",
      "17.1%\n",
      "17.2%\n",
      "17.4%\n",
      "17.5%\n",
      "17.6%\n",
      "17.7%\n",
      "17.9%\n",
      "18.0%\n",
      "18.1%\n",
      "18.2%\n",
      "18.4%\n",
      "18.5%\n",
      "18.6%\n",
      "18.7%\n",
      "18.9%\n",
      "19.0%\n",
      "19.1%\n",
      "19.2%\n",
      "19.3%\n",
      "19.5%\n",
      "19.6%\n",
      "19.7%\n",
      "19.8%\n",
      "20.0%\n",
      "20.1%\n",
      "20.2%\n",
      "20.3%\n",
      "20.5%\n",
      "20.6%\n",
      "20.7%\n",
      "20.8%\n",
      "21.0%\n",
      "21.1%\n",
      "21.2%\n",
      "21.3%\n",
      "21.5%\n",
      "21.6%\n",
      "21.7%\n",
      "21.8%\n",
      "22.0%\n",
      "22.1%\n",
      "22.2%\n",
      "22.3%\n",
      "22.4%\n",
      "22.6%\n",
      "22.7%\n",
      "22.8%\n",
      "22.9%\n",
      "23.1%\n",
      "23.2%\n",
      "23.3%\n",
      "23.4%\n",
      "23.6%\n",
      "23.7%\n",
      "23.8%\n",
      "23.9%\n",
      "24.1%\n",
      "24.2%\n",
      "24.3%\n",
      "24.4%\n",
      "24.6%\n",
      "24.7%\n",
      "24.8%\n",
      "24.9%\n",
      "25.1%\n",
      "25.2%\n",
      "25.3%\n",
      "25.4%\n",
      "25.5%\n",
      "25.7%\n",
      "25.8%\n",
      "25.9%\n",
      "26.0%\n",
      "26.2%\n",
      "26.3%\n",
      "26.4%\n",
      "26.5%\n",
      "26.7%\n",
      "26.8%\n",
      "26.9%\n",
      "27.0%\n",
      "27.2%\n",
      "27.3%\n",
      "27.4%\n",
      "27.5%\n",
      "27.7%\n",
      "27.8%\n",
      "27.9%\n",
      "28.0%\n",
      "28.2%\n",
      "28.3%\n",
      "28.4%\n",
      "28.5%\n",
      "28.6%\n",
      "28.8%\n",
      "28.9%\n",
      "29.0%\n",
      "29.1%\n",
      "29.3%\n",
      "29.4%\n",
      "29.5%\n",
      "29.6%\n",
      "29.8%\n",
      "29.9%\n",
      "30.0%\n",
      "30.1%\n",
      "30.3%\n",
      "30.4%\n",
      "30.5%\n",
      "30.6%\n",
      "30.8%\n",
      "30.9%\n",
      "31.0%\n",
      "31.1%\n",
      "31.3%\n",
      "31.4%\n",
      "31.5%\n",
      "31.6%\n",
      "31.7%\n",
      "31.9%\n",
      "32.0%\n",
      "32.1%\n",
      "32.2%\n",
      "32.4%\n",
      "32.5%\n",
      "32.6%\n",
      "32.7%\n",
      "32.9%\n",
      "33.0%\n",
      "33.1%\n",
      "33.2%\n",
      "33.4%\n",
      "33.5%\n",
      "33.6%\n",
      "33.7%\n",
      "33.9%\n",
      "34.0%\n",
      "34.1%\n",
      "34.2%\n",
      "34.4%\n",
      "34.5%\n",
      "34.6%\n",
      "34.7%\n",
      "34.8%\n",
      "35.0%\n",
      "35.1%\n",
      "35.2%\n",
      "35.3%\n",
      "35.5%\n",
      "35.6%\n",
      "35.7%\n",
      "35.8%\n",
      "36.0%\n",
      "36.1%\n",
      "36.2%\n",
      "36.3%\n",
      "36.5%\n",
      "36.6%\n",
      "36.7%\n",
      "36.8%\n",
      "37.0%\n",
      "37.1%\n",
      "37.2%\n",
      "37.3%\n",
      "37.5%\n",
      "37.6%\n",
      "37.7%\n",
      "37.8%\n",
      "37.9%\n",
      "38.1%\n",
      "38.2%\n",
      "38.3%\n",
      "38.4%\n",
      "38.6%\n",
      "38.7%\n",
      "38.8%\n",
      "38.9%\n",
      "39.1%\n",
      "39.2%\n",
      "39.3%\n",
      "39.4%\n",
      "39.6%\n",
      "39.7%\n",
      "39.8%\n",
      "39.9%\n",
      "40.1%\n",
      "40.2%\n",
      "40.3%\n",
      "40.4%\n",
      "40.6%\n",
      "40.7%\n",
      "40.8%\n",
      "40.9%\n",
      "41.1%\n",
      "41.2%\n",
      "41.3%\n",
      "41.4%\n",
      "41.5%\n",
      "41.7%\n",
      "41.8%\n",
      "41.9%\n",
      "42.0%\n",
      "42.2%\n",
      "42.3%\n",
      "42.4%\n",
      "42.5%\n",
      "42.7%\n",
      "42.8%\n",
      "42.9%\n",
      "43.0%\n",
      "43.2%\n",
      "43.3%\n",
      "43.4%\n",
      "43.5%\n",
      "43.7%\n",
      "43.8%\n",
      "43.9%\n",
      "44.0%\n",
      "44.2%\n",
      "44.3%\n",
      "44.4%\n",
      "44.5%\n",
      "44.6%\n",
      "44.8%\n",
      "44.9%\n",
      "45.0%\n",
      "45.1%\n",
      "45.3%\n",
      "45.4%\n",
      "45.5%\n",
      "45.6%\n",
      "45.8%\n",
      "45.9%\n",
      "46.0%\n",
      "46.1%\n",
      "46.3%\n",
      "46.4%\n",
      "46.5%\n",
      "46.6%\n",
      "46.8%\n",
      "46.9%\n",
      "47.0%\n",
      "47.1%\n",
      "47.3%\n",
      "47.4%\n",
      "47.5%\n",
      "47.6%\n",
      "47.7%\n",
      "47.9%\n",
      "48.0%\n",
      "48.1%\n",
      "48.2%\n",
      "48.4%\n",
      "48.5%\n",
      "48.6%\n",
      "48.7%\n",
      "48.9%\n",
      "49.0%\n",
      "49.1%\n",
      "49.2%\n",
      "49.4%\n",
      "49.5%\n",
      "49.6%\n",
      "49.7%\n",
      "49.9%\n",
      "50.0%\n",
      "50.1%\n",
      "50.2%\n",
      "50.4%\n",
      "50.5%\n",
      "50.6%\n",
      "50.7%\n",
      "50.8%\n",
      "51.0%\n",
      "51.1%\n",
      "51.2%\n",
      "51.3%\n",
      "51.5%\n",
      "51.6%\n",
      "51.7%\n",
      "51.8%\n",
      "52.0%\n",
      "52.1%\n",
      "52.2%\n",
      "52.3%\n",
      "52.5%\n",
      "52.6%\n",
      "52.7%\n",
      "52.8%\n",
      "53.0%\n",
      "53.1%\n",
      "53.2%\n",
      "53.3%\n",
      "53.5%\n",
      "53.6%\n",
      "53.7%\n",
      "53.8%\n",
      "53.9%\n",
      "54.1%\n",
      "54.2%\n",
      "54.3%\n",
      "54.4%\n",
      "54.6%\n",
      "54.7%\n",
      "54.8%\n",
      "54.9%\n",
      "55.1%\n",
      "55.2%\n",
      "55.3%\n",
      "55.4%\n",
      "55.6%\n",
      "55.7%\n",
      "55.8%\n",
      "55.9%\n",
      "56.1%\n",
      "56.2%\n",
      "56.3%\n",
      "56.4%\n",
      "56.6%\n",
      "56.7%\n",
      "56.8%\n",
      "56.9%\n",
      "57.0%\n",
      "57.2%\n",
      "57.3%\n",
      "57.4%\n",
      "57.5%\n",
      "57.7%\n",
      "57.8%\n",
      "57.9%\n",
      "58.0%\n",
      "58.2%\n",
      "58.3%\n",
      "58.4%\n",
      "58.5%\n",
      "58.7%\n",
      "58.8%\n",
      "58.9%\n",
      "59.0%\n",
      "59.2%\n",
      "59.3%\n",
      "59.4%\n",
      "59.5%\n",
      "59.7%\n",
      "59.8%\n",
      "59.9%\n",
      "60.0%\n",
      "60.1%\n",
      "60.3%\n",
      "60.4%\n",
      "60.5%\n",
      "60.6%\n",
      "60.8%\n",
      "60.9%\n",
      "61.0%\n",
      "61.1%\n",
      "61.3%\n",
      "61.4%\n",
      "61.5%\n",
      "61.6%\n",
      "61.8%\n",
      "61.9%\n",
      "62.0%\n",
      "62.1%\n",
      "62.3%\n",
      "62.4%\n",
      "62.5%\n",
      "62.6%\n",
      "62.8%\n",
      "62.9%\n",
      "63.0%\n",
      "63.1%\n",
      "63.2%\n",
      "63.4%\n",
      "63.5%\n",
      "63.6%\n",
      "63.7%\n",
      "63.9%\n",
      "64.0%\n",
      "64.1%\n",
      "64.2%\n",
      "64.4%\n",
      "64.5%\n",
      "64.6%\n",
      "64.7%\n",
      "64.9%\n",
      "65.0%\n",
      "65.1%\n",
      "65.2%\n",
      "65.4%\n",
      "65.5%\n",
      "65.6%\n",
      "65.7%\n",
      "65.9%\n",
      "66.0%\n",
      "66.1%\n",
      "66.2%\n",
      "66.3%\n",
      "66.5%\n",
      "66.6%\n",
      "66.7%\n",
      "66.8%\n",
      "67.0%\n",
      "67.1%\n",
      "67.2%\n",
      "67.3%\n",
      "67.5%\n",
      "67.6%\n",
      "67.7%\n",
      "67.8%\n",
      "68.0%\n",
      "68.1%\n",
      "68.2%\n",
      "68.3%\n",
      "68.5%\n",
      "68.6%\n",
      "68.7%\n",
      "68.8%\n",
      "69.0%\n",
      "69.1%\n",
      "69.2%\n",
      "69.3%\n",
      "69.5%\n",
      "69.6%\n",
      "69.7%\n",
      "69.8%\n",
      "69.9%\n",
      "70.1%\n",
      "70.2%\n",
      "70.3%\n",
      "70.4%\n",
      "70.6%\n",
      "70.7%\n",
      "70.8%\n",
      "70.9%\n",
      "71.1%\n",
      "71.2%\n",
      "71.3%\n",
      "71.4%\n",
      "71.6%\n",
      "71.7%\n",
      "71.8%\n",
      "71.9%\n",
      "72.1%\n",
      "72.2%\n",
      "72.3%\n",
      "72.4%\n",
      "72.6%\n",
      "72.7%\n",
      "72.8%\n",
      "72.9%\n",
      "73.0%\n",
      "73.2%\n",
      "73.3%\n",
      "73.4%\n",
      "73.5%\n",
      "73.7%\n",
      "73.8%\n",
      "73.9%\n",
      "74.0%\n",
      "74.2%\n",
      "74.3%\n",
      "74.4%\n",
      "74.5%\n",
      "74.7%\n",
      "74.8%\n",
      "74.9%\n",
      "75.0%\n",
      "75.2%\n",
      "75.3%\n",
      "75.4%\n",
      "75.5%\n",
      "75.7%\n",
      "75.8%\n",
      "75.9%\n",
      "76.0%\n",
      "76.1%\n",
      "76.3%\n",
      "76.4%\n",
      "76.5%\n",
      "76.6%\n",
      "76.8%\n",
      "76.9%\n",
      "77.0%\n",
      "77.1%\n",
      "77.3%\n",
      "77.4%\n",
      "77.5%\n",
      "77.6%\n",
      "77.8%\n",
      "77.9%\n",
      "78.0%\n",
      "78.1%\n",
      "78.3%\n",
      "78.4%\n",
      "78.5%\n",
      "78.6%\n",
      "78.8%\n",
      "78.9%\n",
      "79.0%\n",
      "79.1%\n",
      "79.2%\n",
      "79.4%\n",
      "79.5%\n",
      "79.6%\n",
      "79.7%\n",
      "79.9%\n",
      "80.0%\n",
      "80.1%\n",
      "80.2%\n",
      "80.4%\n",
      "80.5%\n",
      "80.6%\n",
      "80.7%\n",
      "80.9%\n",
      "81.0%\n",
      "81.1%\n",
      "81.2%\n",
      "81.4%\n",
      "81.5%\n",
      "81.6%\n",
      "81.7%\n",
      "81.9%\n",
      "82.0%\n",
      "82.1%\n",
      "82.2%\n",
      "82.3%\n",
      "82.5%\n",
      "82.6%\n",
      "82.7%\n",
      "82.8%\n",
      "83.0%\n",
      "83.1%\n",
      "83.2%\n",
      "83.3%\n",
      "83.5%\n",
      "83.6%\n",
      "83.7%\n",
      "83.8%\n",
      "84.0%\n",
      "84.1%\n",
      "84.2%\n",
      "84.3%\n",
      "84.5%\n",
      "84.6%\n",
      "84.7%\n",
      "84.8%\n",
      "85.0%\n",
      "85.1%\n",
      "85.2%\n",
      "85.3%\n",
      "85.4%\n",
      "85.6%\n",
      "85.7%\n",
      "85.8%\n",
      "85.9%\n",
      "86.1%\n",
      "86.2%\n",
      "86.3%\n",
      "86.4%\n",
      "86.6%\n",
      "86.7%\n",
      "86.8%\n",
      "86.9%\n",
      "87.1%\n",
      "87.2%\n",
      "87.3%\n",
      "87.4%\n",
      "87.6%\n",
      "87.7%\n",
      "87.8%\n",
      "87.9%\n",
      "88.1%\n",
      "88.2%\n",
      "88.3%\n",
      "88.4%\n",
      "88.5%\n",
      "88.7%\n",
      "88.8%\n",
      "88.9%\n",
      "89.0%\n",
      "89.2%\n",
      "89.3%\n",
      "89.4%\n",
      "89.5%\n",
      "89.7%\n",
      "89.8%\n",
      "89.9%\n",
      "90.0%\n",
      "90.2%\n",
      "90.3%\n",
      "90.4%\n",
      "90.5%\n",
      "90.7%\n",
      "90.8%\n",
      "90.9%\n",
      "91.0%\n",
      "91.2%\n",
      "91.3%\n",
      "91.4%\n",
      "91.5%\n",
      "91.6%\n",
      "91.8%\n",
      "91.9%\n",
      "92.0%\n",
      "92.1%\n",
      "92.3%\n",
      "92.4%\n",
      "92.5%\n",
      "92.6%\n",
      "92.8%\n",
      "92.9%\n",
      "93.0%\n",
      "93.1%\n",
      "93.3%\n",
      "93.4%\n",
      "93.5%\n",
      "93.6%\n",
      "93.8%\n",
      "93.9%\n",
      "94.0%\n",
      "94.1%\n",
      "94.3%\n",
      "94.4%\n",
      "94.5%\n",
      "94.6%\n",
      "94.8%\n",
      "94.9%\n",
      "95.0%\n",
      "95.1%\n",
      "95.2%\n",
      "95.4%\n",
      "95.5%\n",
      "95.6%\n",
      "95.7%\n",
      "95.9%\n",
      "96.0%\n",
      "96.1%\n",
      "96.2%\n",
      "96.4%\n",
      "96.5%\n",
      "96.6%\n",
      "96.7%\n",
      "96.9%\n",
      "97.0%\n",
      "97.1%\n",
      "97.2%\n",
      "97.4%\n",
      "97.5%\n",
      "97.6%\n",
      "97.7%\n",
      "97.9%\n",
      "98.0%\n",
      "98.1%\n",
      "98.2%\n",
      "98.3%\n",
      "98.5%\n",
      "98.6%\n",
      "98.7%\n",
      "98.8%\n",
      "99.0%\n",
      "99.1%\n",
      "99.2%\n",
      "99.3%\n",
      "99.5%\n",
      "99.6%\n",
      "99.7%\n",
      "99.8%\n",
      "100.0%\n",
      "100.0%\n",
      "\n",
      "100.0%\n",
      "\n",
      "0.7%\n",
      "1.5%\n",
      "2.2%\n",
      "3.0%\n",
      "3.7%\n",
      "4.4%\n",
      "5.2%\n",
      "5.9%\n",
      "6.7%\n",
      "7.4%\n",
      "8.2%\n",
      "8.9%\n",
      "9.6%\n",
      "10.4%\n",
      "11.1%\n",
      "11.9%\n",
      "12.6%\n",
      "13.3%\n",
      "14.1%\n",
      "14.8%\n",
      "15.6%\n",
      "16.3%\n",
      "17.0%\n",
      "17.8%\n",
      "18.5%\n",
      "19.3%\n",
      "20.0%\n",
      "20.7%\n",
      "21.5%\n",
      "22.2%\n",
      "23.0%\n",
      "23.7%\n",
      "24.5%\n",
      "25.2%\n",
      "25.9%\n",
      "26.7%\n",
      "27.4%\n",
      "28.2%\n",
      "28.9%\n",
      "29.6%\n",
      "30.4%\n",
      "31.1%\n",
      "31.9%\n",
      "32.6%\n",
      "33.3%\n",
      "34.1%\n",
      "34.8%\n",
      "35.6%\n",
      "36.3%\n",
      "37.1%\n",
      "37.8%\n",
      "38.5%\n",
      "39.3%\n",
      "40.0%\n",
      "40.8%\n",
      "41.5%\n",
      "42.2%\n",
      "43.0%\n",
      "43.7%\n",
      "44.5%\n",
      "45.2%\n",
      "45.9%\n",
      "46.7%\n",
      "47.4%\n",
      "48.2%\n",
      "48.9%\n",
      "49.6%\n",
      "50.4%\n",
      "51.1%\n",
      "51.9%\n",
      "52.6%\n",
      "53.4%\n",
      "54.1%\n",
      "54.8%\n",
      "55.6%\n",
      "56.3%\n",
      "57.1%\n",
      "57.8%\n",
      "58.5%\n",
      "59.3%\n",
      "60.0%\n",
      "60.8%\n",
      "61.5%\n",
      "62.2%\n",
      "63.0%\n",
      "63.7%\n",
      "64.5%\n",
      "65.2%\n",
      "65.9%\n",
      "66.7%\n",
      "67.4%\n",
      "68.2%\n",
      "68.9%\n",
      "69.7%\n",
      "70.4%\n",
      "71.1%\n",
      "71.9%\n",
      "72.6%\n",
      "73.4%\n",
      "74.1%\n",
      "74.8%\n",
      "75.6%\n",
      "76.3%\n",
      "77.1%\n",
      "77.8%\n",
      "78.5%\n",
      "79.3%\n",
      "80.0%\n",
      "80.8%\n",
      "81.5%\n",
      "82.3%\n",
      "83.0%\n",
      "83.7%\n",
      "84.5%\n",
      "85.2%\n",
      "86.0%\n",
      "86.7%\n",
      "87.4%\n",
      "88.2%\n",
      "88.9%\n",
      "89.7%\n",
      "90.4%\n",
      "91.1%\n",
      "91.9%\n",
      "92.6%\n",
      "93.4%\n",
      "94.1%\n",
      "94.8%\n",
      "95.6%\n",
      "96.3%\n",
      "97.1%\n",
      "97.8%\n",
      "98.6%\n",
      "99.3%\n",
      "100.0%\n",
      "\n",
      "100.0%\n",
      "ðŸ“Š Epoch 1, Batch 0/938, Loss: 2.3023\n",
      "ðŸ“Š Epoch 1, Batch 100/938, Loss: 1.2173\n",
      "ðŸ“Š Epoch 1, Batch 200/938, Loss: 0.8388\n",
      "ðŸ“Š Epoch 1, Batch 300/938, Loss: 0.6253\n",
      "ðŸ“Š Epoch 1, Batch 400/938, Loss: 0.5499\n",
      "ðŸ“Š Epoch 1, Batch 500/938, Loss: 0.6849\n",
      "ðŸ“Š Epoch 1, Batch 600/938, Loss: 0.4825\n",
      "ðŸ“Š Epoch 1, Batch 700/938, Loss: 0.5188\n",
      "ðŸ“Š Epoch 1, Batch 800/938, Loss: 0.3764\n",
      "ðŸ“Š Epoch 1, Batch 900/938, Loss: 0.5170\n",
      "ðŸ“Š Epoch 2, Batch 0/938, Loss: 0.4243\n",
      "ðŸ“Š Epoch 2, Batch 100/938, Loss: 0.6557\n",
      "ðŸ“Š Epoch 2, Batch 200/938, Loss: 0.6961\n",
      "ðŸ“Š Epoch 2, Batch 300/938, Loss: 0.4505\n",
      "ðŸ“Š Epoch 2, Batch 400/938, Loss: 0.4865\n",
      "ðŸ“Š Epoch 2, Batch 500/938, Loss: 0.4271\n",
      "ðŸ“Š Epoch 2, Batch 600/938, Loss: 0.2128\n",
      "ðŸ“Š Epoch 2, Batch 700/938, Loss: 0.4019\n",
      "ðŸ“Š Epoch 2, Batch 800/938, Loss: 0.5162\n",
      "ðŸ“Š Epoch 2, Batch 900/938, Loss: 0.4621\n",
      "âœ… Model saved!\n",
      "ðŸŽ‰ Training complete!\n",
      "[zf5194b02611-train] Completed with code 0 in 0:01:03.909086 seconds.Operating inside /var/folders/r3/kwn1z7n15nq3rh54ykdsy73r0000gn/T/zf5194b02611womr11v5Looking in links: /tmp/tmpdcvyyznzProcessing /tmp/tmpdcvyyznz/pip-24.2-py3-none-any.whlInstalling collected packages: pipSuccessfully installed pip-24.2Collecting torch  Using cached torch-2.9.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)Collecting torchvision  Using cached torchvision-0.24.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.9 kB)Collecting filelock (from torch)  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)Collecting typing-extensions>=4.10.0 (from torch)  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)Collecting setuptools (from torch)  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)Collecting sympy>=1.13.3 (from torch)  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)Collecting networkx>=2.5.1 (from torch)  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)Collecting jinja2 (from torch)  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)Collecting fsspec>=0.8.5 (from torch)  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)Collecting numpy (from torchvision)  Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)  Using cached pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)Collecting MarkupSafe>=2.0 (from jinja2->torch)  Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)Using cached torch-2.9.0-cp313-none-macosx_11_0_arm64.whl (74.5 MB)Using cached torchvision-0.24.0-cp313-cp313-macosx_12_0_arm64.whl (1.9 MB)Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)Using cached networkx-3.5-py3-none-any.whl (2.0 MB)Using cached pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)Using cached filelock-3.20.0-py3-none-any.whl (16 kB)Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)Using cached markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)Installing collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvisionSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.4 pillow-12.0.0 setuptools-80.9.0 sympy-1.14.0 torch-2.9.0 torchvision-0.24.0 typing-extensions-4.15.0W1021 09:57:20.870000 47481 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.ðŸš€ Starting training...0.1%0.2%0.4%0.5%0.6%0.7%0.9%1.0%1.1%1.2%1.4%1.5%1.6%1.7%1.9%2.0%2.1%2.2%2.4%2.5%2.6%2.7%2.9%3.0%3.1%3.2%3.3%3.5%3.6%3.7%3.8%4.0%4.1%4.2%4.3%4.5%4.6%4.7%4.8%5.0%5.1%5.2%5.3%5.5%5.6%5.7%5.8%6.0%6.1%6.2%6.3%6.4%6.6%6.7%6.8%6.9%7.1%7.2%7.3%7.4%7.6%7.7%7.8%7.9%8.1%8.2%8.3%8.4%8.6%8.7%8.8%8.9%9.1%9.2%9.3%9.4%9.5%9.7%9.8%9.9%10.0%10.2%10.3%10.4%10.5%10.7%10.8%10.9%11.0%11.2%11.3%11.4%11.5%11.7%11.8%11.9%12.0%12.2%12.3%12.4%12.5%12.6%12.8%12.9%13.0%13.1%13.3%13.4%13.5%13.6%13.8%13.9%14.0%14.1%14.3%14.4%14.5%14.6%14.8%14.9%15.0%15.1%15.3%15.4%15.5%15.6%15.8%15.9%16.0%16.1%16.2%16.4%16.5%16.6%16.7%16.9%17.0%17.1%17.2%17.4%17.5%17.6%17.7%17.9%18.0%18.1%18.2%18.4%18.5%18.6%18.7%18.9%19.0%19.1%19.2%19.3%19.5%19.6%19.7%19.8%20.0%20.1%20.2%20.3%20.5%20.6%20.7%20.8%21.0%21.1%21.2%21.3%21.5%21.6%21.7%21.8%22.0%22.1%22.2%22.3%22.4%22.6%22.7%22.8%22.9%23.1%23.2%23.3%23.4%23.6%23.7%23.8%23.9%24.1%24.2%24.3%24.4%24.6%24.7%24.8%24.9%25.1%25.2%25.3%25.4%25.5%25.7%25.8%25.9%26.0%26.2%26.3%26.4%26.5%26.7%26.8%26.9%27.0%27.2%27.3%27.4%27.5%27.7%27.8%27.9%28.0%28.2%28.3%28.4%28.5%28.6%28.8%28.9%29.0%29.1%29.3%29.4%29.5%29.6%29.8%29.9%30.0%30.1%30.3%30.4%30.5%30.6%30.8%30.9%31.0%31.1%31.3%31.4%31.5%31.6%31.7%31.9%32.0%32.1%32.2%32.4%32.5%32.6%32.7%32.9%33.0%33.1%33.2%33.4%33.5%33.6%33.7%33.9%34.0%34.1%34.2%34.4%34.5%34.6%34.7%34.8%35.0%35.1%35.2%35.3%35.5%35.6%35.7%35.8%36.0%36.1%36.2%36.3%36.5%36.6%36.7%36.8%37.0%37.1%37.2%37.3%37.5%37.6%37.7%37.8%37.9%38.1%38.2%38.3%38.4%38.6%38.7%38.8%38.9%39.1%39.2%39.3%39.4%39.6%39.7%39.8%39.9%40.1%40.2%40.3%40.4%40.6%40.7%40.8%40.9%41.1%41.2%41.3%41.4%41.5%41.7%41.8%41.9%42.0%42.2%42.3%42.4%42.5%42.7%42.8%42.9%43.0%43.2%43.3%43.4%43.5%43.7%43.8%43.9%44.0%44.2%44.3%44.4%44.5%44.6%44.8%44.9%45.0%45.1%45.3%45.4%45.5%45.6%45.8%45.9%46.0%46.1%46.3%46.4%46.5%46.6%46.8%46.9%47.0%47.1%47.3%47.4%47.5%47.6%47.7%47.9%48.0%48.1%48.2%48.4%48.5%48.6%48.7%48.9%49.0%49.1%49.2%49.4%49.5%49.6%49.7%49.9%50.0%50.1%50.2%50.4%50.5%50.6%50.7%50.8%51.0%51.1%51.2%51.3%51.5%51.6%51.7%51.8%52.0%52.1%52.2%52.3%52.5%52.6%52.7%52.8%53.0%53.1%53.2%53.3%53.5%53.6%53.7%53.8%53.9%54.1%54.2%54.3%54.4%54.6%54.7%54.8%54.9%55.1%55.2%55.3%55.4%55.6%55.7%55.8%55.9%56.1%56.2%56.3%56.4%56.6%56.7%56.8%56.9%57.0%57.2%57.3%57.4%57.5%57.7%57.8%57.9%58.0%58.2%58.3%58.4%58.5%58.7%58.8%58.9%59.0%59.2%59.3%59.4%59.5%59.7%59.8%59.9%60.0%60.1%60.3%60.4%60.5%60.6%60.8%60.9%61.0%61.1%61.3%61.4%61.5%61.6%61.8%61.9%62.0%62.1%62.3%62.4%62.5%62.6%62.8%62.9%63.0%63.1%63.2%63.4%63.5%63.6%63.7%63.9%64.0%64.1%64.2%64.4%64.5%64.6%64.7%64.9%65.0%65.1%65.2%65.4%65.5%65.6%65.7%65.9%66.0%66.1%66.2%66.3%66.5%66.6%66.7%66.8%67.0%67.1%67.2%67.3%67.5%67.6%67.7%67.8%68.0%68.1%68.2%68.3%68.5%68.6%68.7%68.8%69.0%69.1%69.2%69.3%69.5%69.6%69.7%69.8%69.9%70.1%70.2%70.3%70.4%70.6%70.7%70.8%70.9%71.1%71.2%71.3%71.4%71.6%71.7%71.8%71.9%72.1%72.2%72.3%72.4%72.6%72.7%72.8%72.9%73.0%73.2%73.3%73.4%73.5%73.7%73.8%73.9%74.0%74.2%74.3%74.4%74.5%74.7%74.8%74.9%75.0%75.2%75.3%75.4%75.5%75.7%75.8%75.9%76.0%76.1%76.3%76.4%76.5%76.6%76.8%76.9%77.0%77.1%77.3%77.4%77.5%77.6%77.8%77.9%78.0%78.1%78.3%78.4%78.5%78.6%78.8%78.9%79.0%79.1%79.2%79.4%79.5%79.6%79.7%79.9%80.0%80.1%80.2%80.4%80.5%80.6%80.7%80.9%81.0%81.1%81.2%81.4%81.5%81.6%81.7%81.9%82.0%82.1%82.2%82.3%82.5%82.6%82.7%82.8%83.0%83.1%83.2%83.3%83.5%83.6%83.7%83.8%84.0%84.1%84.2%84.3%84.5%84.6%84.7%84.8%85.0%85.1%85.2%85.3%85.4%85.6%85.7%85.8%85.9%86.1%86.2%86.3%86.4%86.6%86.7%86.8%86.9%87.1%87.2%87.3%87.4%87.6%87.7%87.8%87.9%88.1%88.2%88.3%88.4%88.5%88.7%88.8%88.9%89.0%89.2%89.3%89.4%89.5%89.7%89.8%89.9%90.0%90.2%90.3%90.4%90.5%90.7%90.8%90.9%91.0%91.2%91.3%91.4%91.5%91.6%91.8%91.9%92.0%92.1%92.3%92.4%92.5%92.6%92.8%92.9%93.0%93.1%93.3%93.4%93.5%93.6%93.8%93.9%94.0%94.1%94.3%94.4%94.5%94.6%94.8%94.9%95.0%95.1%95.2%95.4%95.5%95.6%95.7%95.9%96.0%96.1%96.2%96.4%96.5%96.6%96.7%96.9%97.0%97.1%97.2%97.4%97.5%97.6%97.7%97.9%98.0%98.1%98.2%98.3%98.5%98.6%98.7%98.8%99.0%99.1%99.2%99.3%99.5%99.6%99.7%99.8%100.0%100.0%100.0%0.7%1.5%2.2%3.0%3.7%4.4%5.2%5.9%6.7%7.4%8.2%8.9%9.6%10.4%11.1%11.9%12.6%13.3%14.1%14.8%15.6%16.3%17.0%17.8%18.5%19.3%20.0%20.7%21.5%22.2%23.0%23.7%24.5%25.2%25.9%26.7%27.4%28.2%28.9%29.6%30.4%31.1%31.9%32.6%33.3%34.1%34.8%35.6%36.3%37.1%37.8%38.5%39.3%40.0%40.8%41.5%42.2%43.0%43.7%44.5%45.2%45.9%46.7%47.4%48.2%48.9%49.6%50.4%51.1%51.9%52.6%53.4%54.1%54.8%55.6%56.3%57.1%57.8%58.5%59.3%60.0%60.8%61.5%62.2%63.0%63.7%64.5%65.2%65.9%66.7%67.4%68.2%68.9%69.7%70.4%71.1%71.9%72.6%73.4%74.1%74.8%75.6%76.3%77.1%77.8%78.5%79.3%80.0%80.8%81.5%82.3%83.0%83.7%84.5%85.2%86.0%86.7%87.4%88.2%88.9%89.7%90.4%91.1%91.9%92.6%93.4%94.1%94.8%95.6%96.3%97.1%97.8%98.6%99.3%100.0%100.0%ðŸ“Š Epoch 1, Batch 0/938, Loss: 2.3023ðŸ“Š Epoch 1, Batch 100/938, Loss: 1.2173ðŸ“Š Epoch 1, Batch 200/938, Loss: 0.8388ðŸ“Š Epoch 1, Batch 300/938, Loss: 0.6253ðŸ“Š Epoch 1, Batch 400/938, Loss: 0.5499ðŸ“Š Epoch 1, Batch 500/938, Loss: 0.6849ðŸ“Š Epoch 1, Batch 600/938, Loss: 0.4825ðŸ“Š Epoch 1, Batch 700/938, Loss: 0.5188ðŸ“Š Epoch 1, Batch 800/938, Loss: 0.3764ðŸ“Š Epoch 1, Batch 900/938, Loss: 0.5170ðŸ“Š Epoch 2, Batch 0/938, Loss: 0.4243ðŸ“Š Epoch 2, Batch 100/938, Loss: 0.6557ðŸ“Š Epoch 2, Batch 200/938, Loss: 0.6961ðŸ“Š Epoch 2, Batch 300/938, Loss: 0.4505ðŸ“Š Epoch 2, Batch 400/938, Loss: 0.4865ðŸ“Š Epoch 2, Batch 500/938, Loss: 0.4271ðŸ“Š Epoch 2, Batch 600/938, Loss: 0.2128ðŸ“Š Epoch 2, Batch 700/938, Loss: 0.4019ðŸ“Š Epoch 2, Batch 800/938, Loss: 0.5162ðŸ“Š Epoch 2, Batch 900/938, Loss: 0.4621âœ… Model saved!ðŸŽ‰ Training complete![zf5194b02611-train] Completed with code 0 in 0:01:03.909086 seconds."
     ]
    }
   ],
   "source": [
    "print(\"Streaming logs (Ctrl+C to stop):\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    for log_line in client.get_job_logs(job_name, follow=True):\n",
    "        print(log_line, end='')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n  Log streaming stopped by user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Completion\n",
    "\n",
    "Wait for the training job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    completed_job = client.wait_for_job_status(\n",
    "        name=job_name,\n",
    "        status={\"Complete\"},\n",
    "        timeout=600,  # 10 minutes\n",
    "        polling_interval=5  # Check every 5 seconds\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Training job completed successfully!\")\n",
    "except TimeoutError:\n",
    "    print(f\"\\n Job did not complete within timeout\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Delete the training job to free up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the job (kills subprocess and cleans up venv)\n",
    "client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
